- 二叉搜索树，根节点的值大于左子树，小于右子树
---
- 插入排序：从后向前循环移动到i-1位，然后将i+1位插入到对应位置
---
- 堆排序：最大堆：根节点大于左右节点值，堆排序包括：
   - 维护堆：通过递归，保证根节点大于左右节点；
   - 建堆：自底向上，初始化树（按照数组元素顺序，逐层赋值），然后从 `[n/2, 1]`区间,循环调用 `维护堆` 的过程，即可将其转化为最大堆；
   - `[n, 2]`区间，交换跟节点（最大值）和尾结点，并将交换过后的尾节点删除(此时即为提取出最大值)，然后对剩余的`A[1,n-1]`调用 `维护堆` 过程，直到循环结束。即得到从小到大的排序数组。
- 堆排序(最大堆)可用于`最大优先队列`，可用于计算机的作业调度。最小堆-`最小优先队列`，可用于基于事件驱动的模拟器。
---
- 归并排序：
---
- 极大似然估计：
> 就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。
> 似然函数输入有两个：x表示某一个具体的数据,`θ`表示模型的参数。`P(X|θ)`,输入x有无数种分布可以选择，极大似然估计应该按照什么原则去选取这个分布呢？
答：采取的方法是`让这个样本结果出现的可能性最大`，这也就是最大似然估计的核心。
极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。
---
- 最大后验概率：
> 最大后验概率估计则是想求`θ`使`P(x0|θ)P(θ)`最大。求得的θ不单单让似然函数大，`θ`自己出现的先验概率也得大.最大化`P(θ|x0)`的意义也很明确，`x0`已经出现了，要求`θ`取什么值使`P(θ|x0)`最大。
   - `MAP最大后验概率`就是多个作为因子的先验概率`P(θ)`。或者，也可以反过来，认为MLE是把先验概率`P(θ)`认为等于1，即认为`θ`是均匀分布。
---
- 各种距离：
    - 欧式距离：
       - $$\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$
    - 曼哈顿距离：
       -  $$|x_1- x_2| + |y_1 - y_2|$$
    - 余弦距离：
         - $$\frac{x_1*x_2 + y_1*y_2}{\sqrt{x_1^2+y_1^2}*\sqrt{x_2^2+y_2^2}}$$
    - 切比雪夫距离:
         - $$max(|x_2 - x_1|, |y_2 - y_1|)$$
---
 - 生成式模型&判别式模型：
   - 生成模型学习一个联合概率分布`P(x，y)`，而判别模型学习一个条件概率分布`P(y|x)`即后验概率。
   - 生成式模型，对于未见示例X，求出与不同类别的之间的联合概率分布，大的获胜，如朴素贝叶斯模型、隐形马尔科夫模型、马尔科夫随机场；
   - 判别式模型，对示例X，根据`P(Y|X)`直接求得标记Y，传统的机器学习算法都是判别式模型，如SVM、LR、神经网络等。
   - 模型举例：
      - 1、要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
      - 2、生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。
---
- LR和线性回归的区别
   - 线性回归用来做预测,LR用来做分类。
   - 线性回归是来拟合函数,LR是来预测函数。
   - 线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。
   - 线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。
   - LR是参数模型，SVM是非参数模型.
   - 损失函数：逻辑回归采用的是logistical loss，SVM采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重
---
- LR损失函数：
   - 对数损失函数：
   $$cost(y_i, p(y|x)) = −∑[ y_i * ln p(y_i|x_i) + (1 − y_i) * ln(1−p(y_i|x_i))]$$
- SVM损失函数：
   - Hinge：
   $$∑[1 - y_i*(w^T*x_i + b)]+ \lambda||w||^2$$
---
- SVM核函数的选择
   - 当样本的特征很多且维数很高时可考虑`线性核函数`;
   - 当样本数量较多,特征较少时,一般手动进行特征的组合再使用`线性核函数`;
   - 当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用`高斯核函数`,因为高斯核函数为一种`局部性`较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有`较少的参数`。
---
- SVM核函数的作用
   - 核函数的作用就是隐含着一个从`低维空间`到`高维空间`的`映射`，而这个映射可以把低维空间中`线性不可分`的两类点变成`线性可分`的.
---
 - SVM和全部数据有关还是和局部数据有关?
   - SVM只和分类界限上的支持向量点有关,换而言之只和局部数据有关。
---
 - ID3、C4.5、CART树:
   - `ID3`决策树优先选择`信息增益`大的属性来对样本进行划分,但是这样的分裂节点方法有一个很大的缺点,当一个属性可取值数目较多时,可能在这个属性对应值下的样本只有一个或者很少个,此时它的信息增益将很高,ID3会认为这个属性很适合划分,但实际情况下较多属性的取值会使模型的泛化能力较差;
   - `C4.5`不采用信息增益作为划分依据,而是采用`信息增益率`作为划分依据。但是仍不能完全解决以上问题,而是有所改善,这个时候引入了CART树;
   - `CART`树,它使用`gini系数`作为节点的分裂依据。
---
 - 为什么高斯核能够拟合无穷维度:
    - 因为将泰勒展开式代入高斯核,将会得到一个无穷维度的映射。
---
 - 跳出局部极小值的方法：
    - 使用多组不同参数值初始化多个神经网络。
    - 模拟退火，每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小(也可能跳出全局最小)，在迭代过程中，接受`次优解`的概率随着时间的退役逐渐降低，从而保证算法稳定。
    - 使用随机梯度下降，计算梯度是加入`随机因素`，即使陷入局部极小点，梯度仍然可能不为零，有机会跳出局部极小值。
---
- 方差&偏差：
 - `方差`是在概率论和统计方差衡量随机变量或一组数据的离散程度的度量方式，方差越大，离散度越大;
  - `偏差`是预测值（估计值）的期望与真实值之间的差距，偏差越大，越偏离真实数据；
---
- MSE `均方误差`：
   - 指参数估计值与参数真值之差平方的期望值，记为MSE。MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。
   $$MSE=\frac{SSE}{n}=\frac{1}{n}\sum_{i=1}^nw_i(y_i−\overline{y_i})^2$$
 - SSE `和方差`:
    - 该参数计算的是拟合数据很原始数据对应点的误差的平方和
    $$SSE=\sum_{i=1}^nw_i(y_i−\overline{y_i})^2$$
 - RMSE `均方根`:
    - 该统计参数，也叫回归系统的拟合标准差，是MSE的平方根
    $$RMSE = \sqrt{MSE}$$
---
 - 信息熵：
 $$Ent(D) = -\sum_{k=1}^n p_k*log_2p_k$$
    - Ent(D)值越小，样本集合D的纯度越高
 - 信息增益：
 $$Gain(D,a) = Ent(D) - \sum_{v=1}^v\frac{|D^v|}{|D|}*Ent(D^v)$$
    - a为某个属性，有v个取值，`D^v`为在a上取值v的样本数量。需要分别计算每个属性取值的`Ent(D^v)`值。
   - `ID3`决策树以信息增益为准则来选择划分属性。
 - 增益率：
$$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$
$$IV(a) = - \sum_{v=1}^v \frac{|D^v|}{|D|}*log_2\frac{|D^v|}{|D|}$$
   - 属性a的可能取值越多，IV(a)的值通常会越大。
   - 增益率对可取值数较少的属性有偏好。C4.5算法先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的属性。
- 基尼指数：
   - 数据集D的基尼值：
$$Gini(D) = \sum_{k=1}^y\sum_{k'!=k}p_kp_k' = 1- \sum_{k=1}^y p_k^2$$
   - 属性a的基尼指数：
$$Gini_\_index(D,a) = \sum_{v=1}^v \frac{|D^v|}{|D|} Gini(D^v)$$
   - 基尼指数反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，指数越小，D的纯度越高。
   - CART树使用基尼指数划分属性。
   - 选择基尼指数小的属性作为划分属性。
---
 - 聚类性能度量：
    - 外部指标：
    数据集`D={x1,x2,...,xm}`, 聚类的簇划分`C= {C1,C2,...,Ck}`, 参考模型给出的簇划分`C*={C1*,C2*,....,Ck*}`,令l表示`C`和`C*`对应的标记向量，定义：
    a = |SS|, SS = {(xi,xj)|li = lj,li* = lj*, i < j}
    b = ...li = lj,li* != lj*, i < j...
    c = ...li != lj,li* = lj*, i < j...
    d = ...li != lj,li* != lj*, i < j...
    其中，a+b+c+d = m(m-1)/2
       - Jaccad系数：
       $$JC=\frac{a}{a+b+c}$$
       - FM指数：
       $$FMI=\sqrt{\frac{a}{a+b}*\frac{a}{a+c}}$$
       - Rand指数：
       $$RI=\frac{2(a+d)}{m(m-1)}$$
    值均在[0,1]之间，越大越好。
---常用聚类算法：
 - k均值聚类：
    - 通过最小化平方误差：
    $$E=\sum_{i=1}^k\sum_{x-C}||x-u_i||^2$$
    其中，ui是簇Ci的均值向量。E越小，簇内相似度越高。
 - 高斯混合聚类
 - 密度聚类
 - 层次聚类：AGNES采用自底向上聚合策略的层次聚类，先将数据每个样本看做一个初始聚类簇，然后算法运行的每一步找出距离最近的两个聚类簇进行合并，知道达到预设的簇数。
 ---
  - Adaboost:
  - 训练多个`弱分类器`，组合成`强分类器`。算法步骤：
     - 始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
     - 训练弱分类器。具体训练过程:如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
     - 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大`分类误差率`小的弱分类器的`权重`，使其在`最终的分类函数`中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。
---
 - 硬连接与软连接：
    - 硬连接：
      - 文件有相同的 inode 及 data block；
      - 指向源文件的inode；
      - 只能对已存在的文件进行创建；
      - 不能交叉文件系统进行硬链接的创建,可能不同文件有相同inode号。；
      - 不能对目录进行创建，只可对文件创建，否则会产生目录环；
      - 删除一个硬链接文件并不影响其他有相同 inode 号的文件。
      - `ln 目标文件 链接文件`

   - 软连接:
      - 软链接有自己的文件属性及权限等；
      - 可对不存在的文件或目录创建软链接；
      - 软链接可交叉文件系统；
      - 软链接可对文件或目录创建；
      - 创建软链接时，链接计数 i_nlink 不会增加；
      - 删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。
      - `ln -s 目标文件 链接文件`
   - ##### 硬链接的文件显示大小跟原文件一样；软链接的文件很小，只有几十个字节；
   - ##### 硬链接的文件的inode跟原文件一样；软连接的文件有自己的inode，跟原文件不一样；


---
## LR逻辑回归推导：
 ##### 1，线性回归：
  $$f(x) = w^T*x + b$$
  如果令:
  $$w = (w, b)$$ $$x = (x, 1)^T$$
  则$$f(x) = w^T*x$$
  > 对于分类问题，回归方程显然无法解决，但是可以通过设定阈值进行求解，比如感知机，大于0为一类，小于0为另一类。
  如果我们需要预测类别标签的概率，概率区间`[0,1]`的连续值，那就需要逻辑回归模型。
##### 2，逻辑回归：
$$f(x) = w^T*x$$
值域为(-oo, +oo),需要一个函数将其转化，比如Sigmod函数：
$$s(x) = \frac{1}{1+e^{-x}}$$
那么：
$$y = s(f(x)) = s(w^T*x) = \frac{1}{1+e^{-w^Tx}}$$
则现在的任务就是获取权重`w`的值。
##### 3，逻辑回归损失函数：
>我们考虑二分类：

标签1和0，$$y_n\in(1,1)$$
假设把任何一个样本看做一个事件，则该事件发生的概率为p，那么模型输出为标签1的概率为p：
$$P_{y=1}=\frac{1}{1+e^{-w^Tx}}$$
$$P_{y=0}=1-p$$
那么：
$$P(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$$
如果共有N个样本，则该N个样本发生的概率为：
$$P_总=P(y_1|x_1)*P(y_2|x_2)...P(y_N|x_N)$$
>注：该方程中只有w未知量。

将两边取对数:
$$F(w) = ln(P_总)=ln(\Pi p^{y_n}*(1-p)^{1-y_n})
=$$
$$\sum(y_n*ln(p)+(1-y_n)*ln(1-p))$$
其中：
$$p=\frac{1}{1+e^{-w^Tx}}$$
>思想为，如何去权重w，使改合事件发生的概率最大，即为最大使然估计MLE。

##### 4，最大释然估计：
`F(w)`可看做是损失函数，使其取得最大值即:
$$w^*=argmaxF(w) = -argminF(w)$$
##### 5，求F(w)的梯度：
对向量$x=(x_1,x_2,...,x_n)$,其梯度为：
$$x^1=(x^1_1,x^1_2,...x^1_n)$$
对于`Ax`来说，其对x求偏导的结果$A^T$,对$x^TA$求偏导的结果为A
所以$$\dot{w^Tx}=x$$
对前面的概率`p`求`w`的导数：
$$
\begin{aligned}
   \dot{p} &= \dot{\frac{1}{1+e^{-w^Tx}}}\\
   &= \dot{\frac{1}{1+e^{-w^Tx}}}*e^{-w^Tx}*(-x）\\
   &= \frac{1}{1+e^{-w^Tx}}*\frac{e^{-w^Tx}}{1+e^{-w^Tx}}*x\\
   &= p(1-p)*x 
\end{aligned}
$$
那么：
$$
\begin{aligned}
\triangledown F(w)&= \triangledown (\sum_{n=1}^N(y_nln(p)+(1-y_n)ln(1-p)))\\
&=\sum_{n=1}^N(y_nln'(p)+(1-y_n)ln'(1-p))\\
&=\sum_{n=1}^N(y_n(1-p)x_n - (1-y_n)px_n\\
&=\sum_{n=1}^N(y_n-p)*x_n
\end{aligned}
$$
即：
$$\triangledown F(w)= \sum_{n=1}^N(y_n-p)*x_n$$

##### 6，梯度下降法and随机梯度下降法：
- 梯度下降法
我们的目标:
$$w^*=argmaxF(w)$$
那么通过第五步求出的梯度，可以更新w，初始化一个w，指定一个步长a，通过不断迭代修改w，从而靠近最大点：
$$w_{t+1}=w_t+a*F(w)^1$$
- 随机梯度下降法：
如果我们能够在每次更新过程中，加入一点点噪声扰动，可能会更加快速地逼近最优值。在SGD中，我们不直接使用F(w)，而是采用另一个输出为随机变量的替代函数`G(w)`,`G(w)`需要满足它的期望值等于`F(w)'`, 相当于这个函数围绕着`F(w)'`的输出值随机波动。

在SGD中，我们每次只要均匀地、随机选取其中一个样本(x,y),用它代表整体样本，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即`E(G(w))=F(w)'`，因此SGD的更新公式为：
$$w_{t+1}=w_t+a*N(y_n-\frac{1}{1+e^{-w^Tx_n}})*x_n$$

##### 7，逻辑回归的可解释性：
逻辑回归最大的特点就是可解释性很强。

在模型训练完成之后，我们获得了一组n维的权重向量 [公式] 跟偏差 b。

对于权重向量 [公式]，它的每一个维度的值，代表了这个维度的特征对于最终分类结果的贡献大小。假如这个维度是正，说明这个特征对于结果是有正向的贡献，那么它的值越大，说明这个特征对于分类为正起到的作用越重要。

对于偏差b (Bias)，一定程度代表了正负两个类别的判定的容易程度。假如b是0，那么正负类别是均匀的。如果b大于0，说明它更容易被分为正类，反之亦然。
根据逻辑回归里的权重向量在每个特征上面的大小，就能够对于每个特征的重要程度有一个量化的清楚的认识，这就是为什么说逻辑回归模型有着很强的解释性的原因。

---

### KNN算法
> K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

- k近邻算法中k的选取:
  -  选取k值以及它的影响：
     -  1,如果我们选取较小的k值，那么就会意味着我们的整体模型会变得复杂，容 易发生过拟合！
        > 我们可以得到k太小会导致过拟合，很容易将一些噪声（如上图离五边形很近的黑色圆点）学习到模型中，而忽略了数据真实的分布！

     -  如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。
        > 如果k=N（N为训练样本的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。
     - 一般做法：
       > 一般选取一个较小的数值，通常采取 交叉验证法来选取最优的k值。（也就是说，选取k值很重要的关键是实验调参，类似于神经网络选取多少层这种，通过调整超参数来得到一个较好的结果）

